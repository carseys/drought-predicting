{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from data_import import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file import: 100%|██████████| 3/3 [00:01<00:00,  2.88it/s]\n"
     ]
    }
   ],
   "source": [
    "oregon_data_dict = oregon_import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_yearly_periodicity(oregon_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wa_df = oregon_data_dict['train_timeseries'].iloc[4:,:].reset_index(inplace=False, drop=True)\n",
    "wa_df = oregon_data_dict['train_timeseries'].copy()\n",
    "wa_df = wa_df[wa_df['fips']==41067]\n",
    "wa_df.drop(columns=['fips'],inplace=True)\n",
    "wa_df = wa_df.iloc[4:,:]\n",
    "wa_df = wa_df.iloc[:-4,:]\n",
    "wa_df['date'] = wa_df['date'].map(pd.Timestamp.timestamp)\n",
    "wa_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = wa_df.iloc[:7, (wa_df.columns != 'score') & (wa_df.columns != 'date')]\n",
    "y_1 = wa_df.iloc[6, wa_df.columns == 'score']\n",
    "date_1 = wa_df.iloc[:7, wa_df.columns == 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = wa_df.iloc[7:14, (wa_df.columns != 'score') & (wa_df.columns != 'date')]\n",
    "y_2 = wa_df.iloc[13, wa_df.columns == 'score']\n",
    "date_2 = wa_df.iloc[7:14, wa_df.columns == 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroughtDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Readying Drought dataset for model.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.X = torch.tensor(df.iloc[:, (df.columns != 'score') & (df.columns != 'date')].values)\n",
    "        self.y = torch.tensor(df.iloc[:, df.columns == 'score'].dropna().values)\n",
    "        self.date = np.array(df.iloc[:, df.columns == 'date'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.y))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        output_X = self.X[7*index:7+7*index]\n",
    "        output_y = self.y[index]\n",
    "        output_date = self.date[7*index:7+7*index]\n",
    "\n",
    "        return output_X, output_y, output_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_dataset = DroughtDataset(wa_df)\n",
    "wa_data_loader = DataLoader(wa_dataset, batch_size = 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "for batch, (soil_info, drought_rating, date) in enumerate(wa_data_loader):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1f68b4015b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "sequence_len = 7\n",
    "input_len = 20 # number of independent variable columns\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_epochs = 15\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim = 1):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h_0=None, c_0=None):\n",
    "        # managing hidden states and cell states\n",
    "        if h_0 is None or c_0 is None:\n",
    "            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)\n",
    "            c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)\n",
    "        \n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        out = self.output_layer(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Model(\n",
      "  (lstm): LSTM(20, 128, num_layers=2, batch_first=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Model(input_dim = input_len, hidden_dim = hidden_size, num_layers = num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss();\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int, model: nn.Module, loss_fn, optimizer, train_data_loader):\n",
    "    \"\"\"\n",
    "    trains the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'num_epochs' : int\n",
    "    'model' : nn.Module\n",
    "    'train_data_loader'\n",
    "    \"\"\"\n",
    "\n",
    "    total_steps = len(train_data_loader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (soil_info, drought_rating, date) in enumerate(train_data_loader):\n",
    "            output = model(soil_info)\n",
    "            loss = loss_fn(output, drought_rating)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch+1)%10 == 0:\n",
    "                print(f'Epoch: {epoch+1}; Batch: {batch+1} / {total_steps}; Loss: {loss.item():>4f}') # rounding the loss\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Batch: 10 / 56; Loss: 1.094113\n",
      "Epoch: 1; Batch: 20 / 56; Loss: 1.200226\n",
      "Epoch: 1; Batch: 30 / 56; Loss: 0.469843\n",
      "Epoch: 1; Batch: 40 / 56; Loss: 0.364610\n",
      "Epoch: 1; Batch: 50 / 56; Loss: 0.552771\n",
      "Epoch: 2; Batch: 10 / 56; Loss: 0.731142\n",
      "Epoch: 2; Batch: 20 / 56; Loss: 0.920895\n",
      "Epoch: 2; Batch: 30 / 56; Loss: 0.242683\n",
      "Epoch: 2; Batch: 40 / 56; Loss: 0.179181\n",
      "Epoch: 2; Batch: 50 / 56; Loss: 0.063849\n",
      "Epoch: 3; Batch: 10 / 56; Loss: 0.557726\n",
      "Epoch: 3; Batch: 20 / 56; Loss: 1.358895\n",
      "Epoch: 3; Batch: 30 / 56; Loss: 0.586044\n",
      "Epoch: 3; Batch: 40 / 56; Loss: 1.226485\n",
      "Epoch: 3; Batch: 50 / 56; Loss: 0.389799\n",
      "Epoch: 4; Batch: 10 / 56; Loss: 0.488915\n",
      "Epoch: 4; Batch: 20 / 56; Loss: 1.467662\n",
      "Epoch: 4; Batch: 30 / 56; Loss: 0.398961\n",
      "Epoch: 4; Batch: 40 / 56; Loss: 0.373019\n",
      "Epoch: 4; Batch: 50 / 56; Loss: 0.454436\n",
      "Epoch: 5; Batch: 10 / 56; Loss: 0.152013\n",
      "Epoch: 5; Batch: 20 / 56; Loss: 0.243428\n",
      "Epoch: 5; Batch: 30 / 56; Loss: 0.306486\n",
      "Epoch: 5; Batch: 40 / 56; Loss: 1.162284\n",
      "Epoch: 5; Batch: 50 / 56; Loss: 0.221763\n",
      "Epoch: 6; Batch: 10 / 56; Loss: 0.593983\n",
      "Epoch: 6; Batch: 20 / 56; Loss: 0.559122\n",
      "Epoch: 6; Batch: 30 / 56; Loss: 0.003663\n",
      "Epoch: 6; Batch: 40 / 56; Loss: 0.100236\n",
      "Epoch: 6; Batch: 50 / 56; Loss: 1.777371\n",
      "Epoch: 7; Batch: 10 / 56; Loss: 0.869145\n",
      "Epoch: 7; Batch: 20 / 56; Loss: 0.445665\n",
      "Epoch: 7; Batch: 30 / 56; Loss: 1.325023\n",
      "Epoch: 7; Batch: 40 / 56; Loss: 0.495558\n",
      "Epoch: 7; Batch: 50 / 56; Loss: 0.524730\n",
      "Epoch: 8; Batch: 10 / 56; Loss: 0.804878\n",
      "Epoch: 8; Batch: 20 / 56; Loss: 0.826675\n",
      "Epoch: 8; Batch: 30 / 56; Loss: 0.682178\n",
      "Epoch: 8; Batch: 40 / 56; Loss: 0.571323\n",
      "Epoch: 8; Batch: 50 / 56; Loss: 0.969768\n",
      "Epoch: 9; Batch: 10 / 56; Loss: 0.889809\n",
      "Epoch: 9; Batch: 20 / 56; Loss: 0.425180\n",
      "Epoch: 9; Batch: 30 / 56; Loss: 0.109837\n",
      "Epoch: 9; Batch: 40 / 56; Loss: 0.130136\n",
      "Epoch: 9; Batch: 50 / 56; Loss: 0.123246\n",
      "Epoch: 10; Batch: 10 / 56; Loss: 1.287952\n",
      "Epoch: 10; Batch: 20 / 56; Loss: 0.608649\n",
      "Epoch: 10; Batch: 30 / 56; Loss: 0.203485\n",
      "Epoch: 10; Batch: 40 / 56; Loss: 0.916106\n",
      "Epoch: 10; Batch: 50 / 56; Loss: 0.230427\n",
      "Epoch: 11; Batch: 10 / 56; Loss: 0.722041\n",
      "Epoch: 11; Batch: 20 / 56; Loss: 0.452451\n",
      "Epoch: 11; Batch: 30 / 56; Loss: 0.365750\n",
      "Epoch: 11; Batch: 40 / 56; Loss: 1.113673\n",
      "Epoch: 11; Batch: 50 / 56; Loss: 1.319210\n",
      "Epoch: 12; Batch: 10 / 56; Loss: 0.552022\n",
      "Epoch: 12; Batch: 20 / 56; Loss: 1.043620\n",
      "Epoch: 12; Batch: 30 / 56; Loss: 0.216095\n",
      "Epoch: 12; Batch: 40 / 56; Loss: 0.069262\n",
      "Epoch: 12; Batch: 50 / 56; Loss: 1.191870\n",
      "Epoch: 13; Batch: 10 / 56; Loss: 0.955741\n",
      "Epoch: 13; Batch: 20 / 56; Loss: 0.551792\n",
      "Epoch: 13; Batch: 30 / 56; Loss: 1.173955\n",
      "Epoch: 13; Batch: 40 / 56; Loss: 0.452496\n",
      "Epoch: 13; Batch: 50 / 56; Loss: 0.403787\n",
      "Epoch: 14; Batch: 10 / 56; Loss: 1.184481\n",
      "Epoch: 14; Batch: 20 / 56; Loss: 0.877442\n",
      "Epoch: 14; Batch: 30 / 56; Loss: 0.916054\n",
      "Epoch: 14; Batch: 40 / 56; Loss: 0.596722\n",
      "Epoch: 14; Batch: 50 / 56; Loss: 0.357262\n",
      "Epoch: 15; Batch: 10 / 56; Loss: 0.896455\n",
      "Epoch: 15; Batch: 20 / 56; Loss: 1.922860\n",
      "Epoch: 15; Batch: 30 / 56; Loss: 0.637120\n",
      "Epoch: 15; Batch: 40 / 56; Loss: 1.610956\n",
      "Epoch: 15; Batch: 50 / 56; Loss: 0.060384\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=num_epochs, model=model, loss_fn=loss_function, optimizer=optimizer, train_data_loader=wa_data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'wa_county_lstm.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
