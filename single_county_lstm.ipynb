{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "from data_import import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "file import: 100%|██████████| 3/3 [00:00<00:00,  4.36it/s]\n"
     ]
    }
   ],
   "source": [
    "oregon_data_dict = oregon_import()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wa_df = oregon_data_dict['train_timeseries'].iloc[4:,:].reset_index(inplace=False, drop=True)\n",
    "wa_df = oregon_data_dict['train_timeseries'].copy()\n",
    "wa_df = wa_df[wa_df['fips']==41067]\n",
    "wa_df.drop(columns=['fips'],inplace=True)\n",
    "wa_df = wa_df.iloc[4:,:]\n",
    "wa_df = wa_df.iloc[:-4,:]\n",
    "wa_df['date'] = wa_df['date'].map(pd.Timestamp.timestamp)\n",
    "wa_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = wa_df.iloc[:7, (wa_df.columns != 'score') & (wa_df.columns != 'date')]\n",
    "y_1 = wa_df.iloc[6, wa_df.columns == 'score']\n",
    "date_1 = wa_df.iloc[:7, wa_df.columns == 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2 = wa_df.iloc[7:14, (wa_df.columns != 'score') & (wa_df.columns != 'date')]\n",
    "y_2 = wa_df.iloc[13, wa_df.columns == 'score']\n",
    "date_2 = wa_df.iloc[7:14, wa_df.columns == 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroughtDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Readying Drought dataset for model.\n",
    "    \"\"\"\n",
    "    def __init__(self, df):\n",
    "        self.X = torch.tensor(df.iloc[:, (df.columns != 'score') & (df.columns != 'date')].values)\n",
    "        self.y = torch.tensor(df.iloc[:, df.columns == 'score'].dropna().values)\n",
    "        self.date = np.array(df.iloc[:, df.columns == 'date'].values)\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.y))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        output_X = self.X[7*index:7+7*index]\n",
    "        output_y = self.y[index]\n",
    "        output_date = self.date[7*index:7+7*index]\n",
    "\n",
    "        return output_X, output_y, output_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "wa_dataset = DroughtDataset(wa_df)\n",
    "wa_data_loader = DataLoader(wa_dataset, batch_size = 16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "for batch, (soil_info, drought_rating, date) in enumerate(wa_data_loader):\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x24f84975550>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "sequence_len = 7\n",
    "input_len = 18 # number of independent variable columns\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "num_epochs = 15\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim = 1):\n",
    "        super(LSTM_Model, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h_0=None, c_0=None):\n",
    "        # managing hidden states and cell states\n",
    "        if h_0 is None or c_0 is None:\n",
    "            h_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)\n",
    "            c_0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim)\n",
    "        \n",
    "        out, (h_n, c_n) = self.lstm(x, (h_0, c_0))\n",
    "        out = self.output_layer(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_Model(\n",
      "  (lstm): LSTM(18, 128, num_layers=2, batch_first=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LSTM_Model(input_dim = input_len, hidden_dim = hidden_size, num_layers = num_layers)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pears\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "loss_function = nn.MSELoss();\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_epochs: int, model: nn.Module, loss_fn, optimizer, train_data_loader):\n",
    "    \"\"\"\n",
    "    trains the model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    'num_epochs' : int\n",
    "    'model' : nn.Module\n",
    "    'train_data_loader'\n",
    "    \"\"\"\n",
    "\n",
    "    total_steps = len(train_data_loader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (soil_info, drought_rating, date) in enumerate(train_data_loader):\n",
    "            output = model(soil_info)\n",
    "            loss = loss_fn(output, drought_rating)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if (batch+1)%10 == 0:\n",
    "                print(f'Epoch: {epoch+1}; Batch: {batch+1} / {total_steps}; Loss: {loss.item():>4f}') # rounding the loss\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Batch: 10 / 56; Loss: 0.938597\n",
      "Epoch: 1; Batch: 20 / 56; Loss: 1.208475\n",
      "Epoch: 1; Batch: 30 / 56; Loss: 1.680801\n",
      "Epoch: 1; Batch: 40 / 56; Loss: 1.278661\n",
      "Epoch: 1; Batch: 50 / 56; Loss: 0.796126\n",
      "Epoch: 2; Batch: 10 / 56; Loss: 0.603793\n",
      "Epoch: 2; Batch: 20 / 56; Loss: 0.870318\n",
      "Epoch: 2; Batch: 30 / 56; Loss: 1.188012\n",
      "Epoch: 2; Batch: 40 / 56; Loss: 0.939582\n",
      "Epoch: 2; Batch: 50 / 56; Loss: 0.655015\n",
      "Epoch: 3; Batch: 10 / 56; Loss: 1.625801\n",
      "Epoch: 3; Batch: 20 / 56; Loss: 0.603285\n",
      "Epoch: 3; Batch: 30 / 56; Loss: 0.341574\n",
      "Epoch: 3; Batch: 40 / 56; Loss: 0.846237\n",
      "Epoch: 3; Batch: 50 / 56; Loss: 0.650849\n",
      "Epoch: 4; Batch: 10 / 56; Loss: 0.727337\n",
      "Epoch: 4; Batch: 20 / 56; Loss: 0.546364\n",
      "Epoch: 4; Batch: 30 / 56; Loss: 0.699942\n",
      "Epoch: 4; Batch: 40 / 56; Loss: 1.542431\n",
      "Epoch: 4; Batch: 50 / 56; Loss: 0.527385\n",
      "Epoch: 5; Batch: 10 / 56; Loss: 1.559371\n",
      "Epoch: 5; Batch: 20 / 56; Loss: 1.344074\n",
      "Epoch: 5; Batch: 30 / 56; Loss: 0.765897\n",
      "Epoch: 5; Batch: 40 / 56; Loss: 0.610124\n",
      "Epoch: 5; Batch: 50 / 56; Loss: 0.395862\n",
      "Epoch: 6; Batch: 10 / 56; Loss: 0.825229\n",
      "Epoch: 6; Batch: 20 / 56; Loss: 1.114478\n",
      "Epoch: 6; Batch: 30 / 56; Loss: 0.190824\n",
      "Epoch: 6; Batch: 40 / 56; Loss: 0.600612\n",
      "Epoch: 6; Batch: 50 / 56; Loss: 0.587215\n",
      "Epoch: 7; Batch: 10 / 56; Loss: 0.138995\n",
      "Epoch: 7; Batch: 20 / 56; Loss: 0.271999\n",
      "Epoch: 7; Batch: 30 / 56; Loss: 0.859325\n",
      "Epoch: 7; Batch: 40 / 56; Loss: 0.810247\n",
      "Epoch: 7; Batch: 50 / 56; Loss: 1.946472\n",
      "Epoch: 8; Batch: 10 / 56; Loss: 1.177677\n",
      "Epoch: 8; Batch: 20 / 56; Loss: 0.777970\n",
      "Epoch: 8; Batch: 30 / 56; Loss: 0.140225\n",
      "Epoch: 8; Batch: 40 / 56; Loss: 0.835454\n",
      "Epoch: 8; Batch: 50 / 56; Loss: 1.322676\n",
      "Epoch: 9; Batch: 10 / 56; Loss: 0.156573\n",
      "Epoch: 9; Batch: 20 / 56; Loss: 0.324135\n",
      "Epoch: 9; Batch: 30 / 56; Loss: 0.162232\n",
      "Epoch: 9; Batch: 40 / 56; Loss: 0.926587\n",
      "Epoch: 9; Batch: 50 / 56; Loss: 0.536347\n",
      "Epoch: 10; Batch: 10 / 56; Loss: 0.486978\n",
      "Epoch: 10; Batch: 20 / 56; Loss: 0.465942\n",
      "Epoch: 10; Batch: 30 / 56; Loss: 1.088105\n",
      "Epoch: 10; Batch: 40 / 56; Loss: 0.563620\n",
      "Epoch: 10; Batch: 50 / 56; Loss: 0.236639\n",
      "Epoch: 11; Batch: 10 / 56; Loss: 1.172045\n",
      "Epoch: 11; Batch: 20 / 56; Loss: 1.188936\n",
      "Epoch: 11; Batch: 30 / 56; Loss: 1.255545\n",
      "Epoch: 11; Batch: 40 / 56; Loss: 0.370922\n",
      "Epoch: 11; Batch: 50 / 56; Loss: 0.810794\n",
      "Epoch: 12; Batch: 10 / 56; Loss: 1.356199\n",
      "Epoch: 12; Batch: 20 / 56; Loss: 0.059555\n",
      "Epoch: 12; Batch: 30 / 56; Loss: 0.924423\n",
      "Epoch: 12; Batch: 40 / 56; Loss: 1.365721\n",
      "Epoch: 12; Batch: 50 / 56; Loss: 1.984701\n",
      "Epoch: 13; Batch: 10 / 56; Loss: 0.579489\n",
      "Epoch: 13; Batch: 20 / 56; Loss: 0.614369\n",
      "Epoch: 13; Batch: 30 / 56; Loss: 1.600174\n",
      "Epoch: 13; Batch: 40 / 56; Loss: 0.677936\n",
      "Epoch: 13; Batch: 50 / 56; Loss: 0.207482\n",
      "Epoch: 14; Batch: 10 / 56; Loss: 0.711241\n",
      "Epoch: 14; Batch: 20 / 56; Loss: 0.325528\n",
      "Epoch: 14; Batch: 30 / 56; Loss: 0.966912\n",
      "Epoch: 14; Batch: 40 / 56; Loss: 0.819448\n",
      "Epoch: 14; Batch: 50 / 56; Loss: 0.263396\n",
      "Epoch: 15; Batch: 10 / 56; Loss: 0.676951\n",
      "Epoch: 15; Batch: 20 / 56; Loss: 0.143852\n",
      "Epoch: 15; Batch: 30 / 56; Loss: 0.685351\n",
      "Epoch: 15; Batch: 40 / 56; Loss: 0.576206\n",
      "Epoch: 15; Batch: 50 / 56; Loss: 1.352579\n"
     ]
    }
   ],
   "source": [
    "train(num_epochs=num_epochs, model=model, loss_fn=loss_function, optimizer=optimizer, train_data_loader=wa_data_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
